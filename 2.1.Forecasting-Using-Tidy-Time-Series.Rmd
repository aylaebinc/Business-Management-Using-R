---
title: "Forecasting and Time Series Analysis Using R: A Tidy Forecasting Exploratory Analysis"
author: "Illarion  Jabine"
date: "11/4/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```


### 1. Required packages:

* tidyverse: collection of useful tools to work with data
* forecast: Methods and tools for displaying and analysing univariate time series forecasts including exponential smoothing via state space models and automatic ARIMA modelling.
* tsibble: Tidy Temporal Data Frames and Tools
* fable,fabletools: Forecasting Models for Tidy Time Series
* feasts: Feature Extraction and Statistics for Time Series
* lubridate: functions to work with date-times and time-spans


### 2. Key terms
 * tsibble 
 * Time Series
 * Forecasting Steps

### 3. Useful Links & books
 $ <https://cran.r-project.org/web/views/TimeSeries.html>: CRAN Task View: Time Series Analysis
 $ <https://otexts.com/fpp2/>: Forecasting: Principles and Practice by Rob J Hyndman and George Athanasopoulos
 $ <https://fable.tidyverts.org/>
 
### 4. Introduction to Tidy Time Series Approach

In "2.Forecasting-Exploratory-Analysis" text I have discussed how xts package, which is a package that extends standard R functionality, can be used together with standard R visualizing tools like plot() and pairs().
R is a rapidly evolving system, with new approaches and packages arriving all the time.
The fable framework provides the tools to evaluate, visualise, and combine models in a workflow consistent with the tidyverse and following so called tidy forecasting workflo: <https://otexts.com/fpp3/a-tidy-forecasting-workflow.html>
The idea is to build an integrated and consistent time series forecasting workflow working in the tidyverse environment. 
The advantages of using "tidy time series"approach:
 1. tsibble object extends tidy data frames (tibble objects) by introducing temporal structure
 2. tsibble object also allows multiple time series to be stored in a single object
 3. you can manipulate tsibble using dplyr methods, i.e. filter, select, mutate, summarize, etc.
 4. it nativily supports ggplot2
 5. "mable" - a model table stores results of various model feeting as one table object.
 6. "fable" - a forecasting table where each row corresponds to one forecast. 

### 5. Load the libraries
Let's first load the libraries.
```{r loading packages, message=FALSE, warning=FALSE}
library(tidyverse)
library(fable)
library(forecast)
library(tsibble)
library(lubridate)
```

### 6. Loading and checking the data

 6.1 Load and check the datasets.
  Dataset1: transactions.csv: detailed daily sales transactions including customer   information, product, amout and sales chanel.
  Dataset2: articles.csv: product hierarchy.
 
```{r load the data and pre-process them}
# Loading data from Rds file
transactions <- read_csv("transactions.csv")
articles <- read_csv("articles.csv")

# Checking if there are any NAs:
apply(transactions,2,anyNA)
apply(articles,2,anyNA)

# There are 6 NAs in Client City transactions. 

sum(is.na(transactions$client_city))

# I need to add the product hierarchy to transactions dataset from articles dataset:
transactions <- inner_join(transactions,articles,by = "article")

```

 6.2. Before creating a tsibble time series object we have to aggregate our data frame in order to have unique observation per each combination of date and categorical variables.
 
```{r aggregating transactions}
transactions <- transactions %>% group_by(article,client_type,client_country,sales_chanel,article_category,date) %>% summarise(amount = sum(amount))

```

 6.3. Now we can create the tsibble time series object:
 
```{r creating tsibble time series object}
ts_tsibble <- tsibble(date = as.Date(transactions$date), 
                  amount = transactions$amount, 
                  article = transactions$article, 
                  client_type = transactions$client_type, 
                  client_country = transactions$client_country, 
                  sales_chanel = transactions$sales_chanel, 
                  article_category = transactions$article_category,
                  index = date,
                  key = c(article,client_type,client_country,sales_chanel,article_category))
```

### 7. Visualising time series using ggplot and tsibble object

Now that we have tsibble time series object we can enjoy the full potential of  ggplot2 and other tools from tidyverse.

```{r}
# Let's create a basic plot
ts_global_plot <- ts_tsibble %>%
  mutate(year = year(date), 
         quater = quarter(date)) %>%
  filter(article == "SNCF" & 
           sales_chanel %in% c("WEB","WER") & 
           year == 2018 & quater == 4) %>%
  ggplot(aes(x = date, y = amount)) +
  geom_line(aes(col = sales_chanel)) +
  xlab("Year") + ylab("Sales") +
  ggtitle("Total Sales")

# To visualize the plot
ts_global_plot

# And now add to this template different elements.
# Here I want to have two pannels with one plot in each:
ts_global_plot + facet_grid(sales_chanel ~ .)
  
```

### 8. Time Series Features

The "tidy time series" framework includes feasts package (FEatures And Statistics from Time Series). The package provides a collection of features, decomposition methods, statistical summaries and graphics functions for the analysing tidy time series data. The output of features() is a value of a feature for all combinations of key columns.
The features include:
 * Any numerical summary computed from a time series, various simple statistics
 * Autocorrelations (ACF features)
 * STL decompositions features
 * Other features including various coefficients and statistical tests (see this link for more details <https://otexts.com/fpp3/features.html>)

```{r time series features}
# I use filter to restrict the number of rows
ts_wer_sncf <- ts_tsibble %>% filter(sales_chanel == "WER" & article == "SNCF")

# simple mean statistics
ts_wer_sncf %>% features(amount, list(mean=mean)) %>% arrange(mean)

# quantile() function
ts_wer_sncf %>% features(amount, quantile, prob=seq(0,1,by=0.25))

# To compute all of the features from the feasts package.
# Calculating the features
all_features <- ts_wer_sncf %>%
  features(amount, feature_set(pkgs="feasts"))

# Visualizing all features
all_features %>%
  select_at(vars(contains("season"), client_type)) %>%
  mutate(
    seasonal_peak_week = glue::glue("Q{seasonal_peak_week+1}"),
    seasonal_trough_week = glue::glue("Q{seasonal_trough_week+1}"),
  ) %>%
  GGally::ggpairs(mapping = aes(colour=client_type))
```
Other features include (source Forecasting: Principles and Practice, Rob J Hyndman and George Athanasopoulos):

* coef_hurst will calculate the Hurst coefficient of a time series which is a measure of “long memory”. A series with long memory will have significant autocorrelations for many lags.
* spectral_entropy will compute the (Shannon) spectral entropy of a time series, which is a measure of how easy the series is to forecast. A series which has strong trend and seasonality (and so is easy to forecast) will have entropy close to 0. A series that is very noisy (and so is difficult to forecast) will have entropy close to 1.
* bp_stat gives the Box-Pierce statistic for testing if a time series is white noise, while bp_pvalue gives the p-value from that test. 
* lb_stat gives the Ljung-Box statistic for testing if a time series is white noise, while lb_pvalue gives the p-value from that test. 
The kth partial autocorrelations measure the relationship between observations k periods apart after removing the effects of observations between them. So the first partial autocorrelation (k = 1 ) is identical to the first autocorrelation, because there is nothing between them to remove. The pacf5 feature contains the sum of squares of the first five partial autocorrelations.
diff1_pacf5 contains the sum of squares of the first five partial autocorrelations from the differenced data.
diff2_pacf5 contains the sum of squares of the first five partial autocorrelations from the differenced data.
season_pacf contains the partial autocorrelation at the first seasonal lag.
kpss_stat gives the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) statistic for testing if a series is stationary, while kpss_pvalue gives the p-value from that test.
pp_stat gives the Phillips-Perron statistic for testing if a series is non-stationary, while pp_pvalue gives the p-value from that test.
ndiffs gives the number of differences required to lead to a stationary series based on the KPSS test. 
nsdiffs gives the number of seasonal differences required to make a series stationary.
var_tiled_mean gives the variances of the “tiled means” (i.e., the means of consecutive non-overlapping blocks of observations). The default tile length is either 10 (for non-seasonal data) or the length of the seasonal period. This is sometimes called the “stability” feature.
var_tiled_var gives the variances of the “tiled variances” (i.e., the variances of consecutive non-overlapping blocks of observations). This is sometimes called the “lumpiness” feature.
shift_level_max finds the largest mean shift between two consecutive sliding windows of the time series. This is useful for finding sudden jumps or drops in a time series.
shift_level_index gives the index at which the largest mean shift occurs.
shift_var_max finds the largest variance shift between two consecutive sliding windows of the time series. This is useful for finding sudden changes in the volatility of a time series.
shift_var_index gives the index at which the largest mean shift occurs
shift_kl_max finds the largest distributional shift (based on the Kulback-Leibler divergence) between two consecutive sliding windows of the time series. This is useful for finding sudden changes in the distribution of a time series.
shift_kl_index gives the index at which the largest KL shift occurs.
n_crossing_points computes the number of times a time series crosses the median.
n_flat_spots computes the number of sections of the data where the series is relatively unchanging.
stat_arch_lm returns the statistic based on the Lagrange Multiplier (LM) test of Engle (1982) for autoregressive conditional heteroscedasticity (ARCH).
